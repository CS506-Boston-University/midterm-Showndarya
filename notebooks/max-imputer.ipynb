{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMtRHcZC8ebSjBhvgLCWC5V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAGzG2Y__EM9","executionInfo":{"status":"ok","timestamp":1679747707674,"user_tz":240,"elapsed":29021,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"81c21dae-3ca9-42e2-bfb9-60613f9969b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.pipeline import FeatureUnion\n","import lightgbm as lgb\n","from scipy.sparse import csr_matrix\n","import joblib\n","\n","# Load the CSV file\n","df = pd.read_csv(\"drive/MyDrive/CS506-DS/midterm/data/train.csv\")\n","df = df.sample(n=20000, random_state=42)\n","print(df.keys())\n","\n","# Convert ReviewId column to int\n","df['Id'] = df['Id'].astype(int)\n","df.drop(columns=[\"ProductId\",\"UserId\",\"Time\"], axis=1, inplace=True)\n","\n","df['Summary'] = df['Summary'].astype(str)\n","df['Text'] = df['Text'].astype(str)\n","\n","# Define the SimpleImputer for filling missing values with the maximum value\n","imputer = SimpleImputer(strategy='most_frequent')\n","XI=df.copy()\n","XI.drop(columns=[\"Text\",\"Summary\"], axis=1, inplace=True)\n","\n","# Apply the imputer to the Id, HelpfulnessNumerator, and HelpfulnessDenominator columns\n","X_imputed = imputer.fit_transform(XI[['Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator']])\n","df[['Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator']] = X_imputed\n","\n","# Define the TfidfVectorizer for character n-grams\n","char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n","\n","# Define the TfidfVectorizer for word n-grams\n","word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3))\n","\n","# Combine the two vectorizers using FeatureUnion\n","vectorizer = FeatureUnion([\n","    ('char', char_vectorizer),\n","    ('word', word_vectorizer)\n","])\n","\n","# Apply the vectorizer to the Summary and Text columns\n","X = vectorizer.fit_transform(df[['Summary', 'Text']].apply(lambda x: ' '.join(x), axis=1))\n","\n","# Apply MaxAbsScaler to the feature matrix\n","scaler = MaxAbsScaler()\n","X_scaled = scaler.fit_transform(X)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Smbo4Koa_GGm","executionInfo":{"status":"ok","timestamp":1679755081737,"user_tz":240,"elapsed":110432,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"4005f893-6291-4684-dd16-5d96e1bb4d7d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Id', 'ProductId', 'UserId', 'HelpfulnessNumerator',\n","       'HelpfulnessDenominator', 'Time', 'Summary', 'Text', 'Score'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["# Define the LightGBM model\n","params = {\n","    'objective': 'regression',\n","    'metric': 'rmse',\n","    'num_leaves': 31, #41 didnt work\n","    'learning_rate': 0.05, #0.03 didnt work\n","    'feature_fraction': 0.9\n","}\n","dtrain = lgb.Dataset(X_scaled, label=df['Score'])\n","model = lgb.train(params, dtrain, num_boost_round=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SwegjrlYCfsc","executionInfo":{"status":"ok","timestamp":1679755720373,"user_tz":240,"elapsed":638641,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"a7ccd193-cb2e-4f59-839a-5165f117791c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 85.895277 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 4488081\n","[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 77000\n","[LightGBM] [Info] Start training from score 3.713600\n"]}]},{"cell_type":"code","source":["# Save the trained model\n","joblib.dump(model, 'lgb_model.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CKOrzTnCcLe","executionInfo":{"status":"ok","timestamp":1679755723026,"user_tz":240,"elapsed":2658,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"0ad64b1f-0d24-4f1d-e467-bf144a645d90"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['lgb_model.pkl']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["X_submission = pd.read_csv(\"drive/MyDrive/CS506-DS/midterm/data/X_test.csv\")\n","X_submission.drop(columns=[\"ProductId\",\"UserId\",\"Time\",\"Score\"], axis=1, inplace=True)\n","\n","X_submission['Summary'] = X_submission['Summary'].astype(str)\n","X_submission['Text'] = X_submission['Text'].astype(str)\n","\n","X_imputed = imputer.fit_transform(X_submission[['Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator']])\n","X_submission[['Id', 'HelpfulnessNumerator', 'HelpfulnessDenominator']] = X_imputed\n","\n","new_X=vectorizer.transform(X_submission['Summary'] + ' ' + X_submission['Text'])\n","new_X_scaled = scaler.transform(new_X)\n","\n","X_submission['Score'] = model.predict(new_X_scaled)\n","\n","submission = X_submission[['Id', 'Score']]\n","submission.to_csv(\"submission.csv\", index=False)\n"],"metadata":{"id":"lwo08kh1_I6A","executionInfo":{"status":"ok","timestamp":1679755791644,"user_tz":240,"elapsed":68620,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}}},"execution_count":15,"outputs":[]}]}
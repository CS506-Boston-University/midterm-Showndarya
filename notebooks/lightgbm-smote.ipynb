{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdAmaxNVCVggx9gnMdU8FJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kju9yrebO4KI","executionInfo":{"status":"ok","timestamp":1679690191926,"user_tz":240,"elapsed":883,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"b74d4829-ec05-437f-c840-cc71f926842b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.pipeline import FeatureUnion\n","from imblearn.over_sampling import SMOTE\n","import lightgbm as lgb\n","\n","# Load the CSV file\n","df=pd.read_csv(\"drive/MyDrive/CS506-DS/midterm/data/train.csv\")\n","df=df.sample(n=10000, random_state=42)\n","print(df.keys())\n","\n","# Convert ReviewId column to int\n","df['Id'] = df['Id'].astype(int)\n","df.drop(columns=[\"ProductId\",\"UserId\",\"Time\"], axis=1, inplace=True)\n","\n","df['Summary'] = df['Summary'].astype(str)\n","df['Text'] = df['Text'].astype(str)\n","\n","# Drop rows with missing values, SMOTE was failing\n","df.dropna(subset=['Score'], inplace=True)\n","\n","# Define the TfidfVectorizer for character n-grams\n","char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n","\n","# Define the TfidfVectorizer for word n-grams\n","word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3))\n","\n","# Combine the two vectorizers using FeatureUnion\n","vectorizer = FeatureUnion([\n","    ('char', char_vectorizer),\n","    ('word', word_vectorizer)\n","])\n","\n","# Apply the vectorizer to the Summary and Text columns\n","X = vectorizer.fit_transform(df[['Summary', 'Text']].apply(lambda x: ' '.join(x), axis=1))\n","\n","# Apply MaxAbsScaler to the feature matrix\n","scaler = MaxAbsScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Apply SMOTE to the feature matrix\n","smote = SMOTE()\n","X_resampled, y_resampled = smote.fit_resample(X_scaled, df['Score'])\n","\n","# Define the LightGBM model\n","params = {\n","    'objective': 'regression',\n","    'metric': 'rmse',\n","    'num_leaves': 31, #41 didnt work\n","    'learning_rate': 0.05, #0.03 didnt work\n","    'feature_fraction': 0.9\n","}\n","dtrain = lgb.Dataset(X_resampled, label=y_resampled)\n","model = lgb.train(params, dtrain, num_boost_round=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVmB13WEO9T-","outputId":"5680e775-0fc2-4208-8df6-ed5ed9ccdc21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Id', 'ProductId', 'UserId', 'HelpfulnessNumerator',\n","       'HelpfulnessDenominator', 'Time', 'Summary', 'Text', 'Score'],\n","      dtype='object')\n"]}]},{"cell_type":"code","source":["X_submission = pd.read_csv(\"drive/MyDrive/CS506-DS/midterm/data/X_test.csv\")\n","X_submission.drop(columns=[\"ProductId\",\"UserId\",\"Time\",\"Score\"], axis=1, inplace=True)\n","\n","X_submission['Summary'] = X_submission['Summary'].astype(str)\n","X_submission['Text'] = X_submission['Text'].astype(str)\n","\n","new_X=vectorizer.transform(X_submission['Summary'] + ' ' + X_submission['Text'])\n","new_X_scaled = scaler.transform(new_X)\n","\n","X_submission['Score'] = model.predict(new_X_scaled)\n","\n","submission = X_submission[['Id', 'Score']]\n","submission.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"URrlGTApO9XI"},"execution_count":null,"outputs":[]}]}
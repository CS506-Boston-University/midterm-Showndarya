{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFMn1pP9fFS3FhULMPYH+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cA0afFN5bgdQ","executionInfo":{"status":"ok","timestamp":1679691860401,"user_tz":240,"elapsed":21054,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"5983984a-adb6-4d7d-de6e-4033ee59e8fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.pipeline import FeatureUnion\n","from imblearn.over_sampling import RandomOverSampler\n","import lightgbm as lgb\n","\n","# Load the CSV file\n","df=pd.read_csv(\"drive/MyDrive/CS506-DS/midterm/data/train.csv\")\n","df=df.sample(n=10000, random_state=42)\n","print(df.keys())\n","\n","# Convert ReviewId column to int\n","df['Id'] = df['Id'].astype(int)\n","df.drop(columns=[\"ProductId\",\"UserId\",\"Time\"], axis=1, inplace=True)\n","\n","df['Summary'] = df['Summary'].astype(str)\n","df['Text'] = df['Text'].astype(str)\n","\n","# Drop rows with missing values\n","df.dropna(subset=['Score'], inplace=True)\n","\n","# Define the TfidfVectorizer for character n-grams\n","char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=5000)\n","\n","# Define the TfidfVectorizer for word n-grams\n","word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_features=5000)\n","\n","# Combine the two vectorizers using FeatureUnion\n","vectorizer = FeatureUnion([\n","    ('char', char_vectorizer),\n","    ('word', word_vectorizer)\n","])\n","\n","# Apply the vectorizer to the Summary and Text columns\n","X = vectorizer.fit_transform(df[['Summary', 'Text']].apply(lambda x: ' '.join(x), axis=1))\n","\n","# Apply MaxAbsScaler to the feature matrix\n","scaler = MaxAbsScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Apply RandomOverSampler to the feature matrix\n","ros = RandomOverSampler(random_state=42)\n","X_resampled, y_resampled = ros.fit_resample(X_scaled, df['Score'])\n","\n","# Define the LightGBM model\n","params = {\n","    'objective': 'regression',\n","    'metric': 'rmse',\n","    'num_leaves': 31,\n","    'learning_rate': 0.05,\n","    'feature_fraction': 0.9\n","}\n","dtrain = lgb.Dataset(X_resampled, label=y_resampled)\n","model = lgb.train(params, dtrain, num_boost_round=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGPRWbzFbjW4","executionInfo":{"status":"ok","timestamp":1679692144565,"user_tz":240,"elapsed":284169,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"c20c4f45-243b-4f3c-ba90-15faad46f092"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['Id', 'ProductId', 'UserId', 'HelpfulnessNumerator',\n","       'HelpfulnessDenominator', 'Time', 'Summary', 'Text', 'Score'],\n","      dtype='object')\n","[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 6.979845 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 1684597\n","[LightGBM] [Info] Number of data points in the train set: 24455, number of used features: 9999\n","[LightGBM] [Info] Start training from score 3.000000\n"]}]},{"cell_type":"code","source":["X_submission = pd.read_csv(\"drive/MyDrive/CS506-DS/midterm/data/X_test.csv\")\n","X_submission.drop(columns=[\"ProductId\",\"UserId\",\"Time\",\"Score\"], axis=1, inplace=True)\n","\n","X_submission['Summary'] = X_submission['Summary'].astype(str)\n","X_submission['Text'] = X_submission['Text'].astype(str)\n","\n","new_X=vectorizer.transform(X_submission['Summary'] + ' ' + X_submission['Text'])\n","new_X_scaled = scaler.transform(new_X)\n","\n","X_submission['Score'] = model.predict(new_X_scaled)\n","\n","submission = X_submission[['Id', 'Score']]\n","submission.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"hobfZcVqblpT","executionInfo":{"status":"ok","timestamp":1679692180014,"user_tz":240,"elapsed":35469,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}}},"execution_count":4,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCY+Cn229tMDjX+IKlVieO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiIx-i6FJcY_","executionInfo":{"status":"ok","timestamp":1679838429826,"user_tz":240,"elapsed":17787,"user":{"displayName":"Showndarya Madhavan","userId":"16229668428797306034"}},"outputId":"7925a1f5-1dad-4dd3-e5fd-e7564de7cac3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AipcmsNeF55w","outputId":"ba5c3dc0-2148-442c-c7b6-f911c9a6c7b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.pipeline import FeatureUnion\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the CSV file using a context manager\n","with open(\"drive/MyDrive/CS506-DS/midterm/data/train.csv\") as f:\n","    df = pd.read_csv(f, nrows=5000, usecols=[\"Id\", \"Summary\", \"Text\", \"Score\",'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time'])\n","df[\"Id\"] = df[\"Id\"].astype(int)\n","\n","# Split the data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(df[[\"Id\", \"Summary\", \"Text\",'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']], df[\"Score\"], test_size=0.2, random_state=42)\n","\n","# Define the TfidfVectorizer for character n-grams\n","char_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 5), min_df=5, max_df=0.8)\n","\n","# Define the TfidfVectorizer for word n-grams\n","word_vectorizer = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), min_df=5, max_df=0.8)\n","\n","# Combine the two vectorizers using FeatureUnion\n","vectorizer = FeatureUnion([(\"char\", char_vectorizer), (\"word\", word_vectorizer)])\n","\n","# Apply the vectorizer to the training and validation sets\n","X_train = vectorizer.fit_transform(X_train[['Summary', 'Text']].apply(lambda x: \" \".join(x), axis=1))\n","X_val = vectorizer.transform(X_val[['Summary', 'Text']].apply(lambda x: \" \".join(x), axis=1))\n","\n","# Apply MaxAbsScaler to the feature matrices\n","scaler = MaxAbsScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","\n","# Define the LightGBM model\n","params = {\n","    \"objective\": \"regression\",\n","    \"metric\": \"rmse\",\n","    \"num_leaves\": 31,\n","    \"learning_rate\": 0.05,\n","    \"feature_fraction\": 0.9\n","}\n","\n","# Perform hyperparameter tuning using GridSearchCV\n","grid_params = {\n","    \"num_leaves\": [15, 31, 63],\n","    \"learning_rate\": [0.01, 0.05, 0.1],\n","    \"feature_fraction\": [0.5, 0.7, 0.9]\n","}\n","grid_search = GridSearchCV(lgb.LGBMRegressor(**params), grid_params, cv=5, n_jobs=-1)\n","grid_search.fit(X_train_scaled, y_train)\n","\n","# Evaluate the model on the validation set\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_val_scaled)\n","rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n","print(\"Validation RMSE:\", rmse)"]}]}